---
title: "454 project progress report 2"
author: "Tina Chen, Kyle Suelflow, Samina Stack"
date: "`r format(Sys.Date(),'%e %B, %Y')`"
output:
   prettydoc::html_pretty:
    theme: cayman
    toc: yes
    bibliography: Library.bib
    
---

```{r setup, include=FALSE, warning=FALSE}

knitr::opts_chunk$set(
  collapse = TRUE, 
  warning = FALSE,
  message = FALSE,
  fig.height = 2.75, 
  fig.width = 4.25,
  fig.align = 'center')

source('Cleaning.R')
```

## Introduction 

In many statistical analyses, the assumption of independently and identically distributed (IID) observations is a common starting point. However, this assumption may not always hold true, particularly in contexts where observations exhibit spatial correlation. For example, we might expect that production patterns look more similar for two households in Minnesota and Wisconsin than households in Minnesota and Virginia because Minnesota and Wisconsin have similar climates, and exogenous factors such as weather may impact the production process. In such scenarios, traditional statistical models may fail to capture the underlying spatial dependencies present in the data.

To address this challenge, spatial models offer a solution by explicitly incorporating spatial relationships into the modeling framework. These models acknowledge that observations in nearby locations tend to be more similar to each other than those in distant locations. By accounting for spatial correlation, spatial models provide a more nuanced understanding of the data and can yield more accurate predictions and inferences.

Bayesian statistical methods serve as a generally more intuitive and flexible alternative to the more standard frequentist methods. They account for prior knowledge, not just the available data, when generating predictions. This facilitates a refinement process as new data becomes available, allowing us to update our predictions using previous predictions and new data. This process also allows for more intuitive interpretation of probabilities as relative plausibility an event occurs rather than the frequency at which it occurs.

In our paper, we want to investigate the integration of Bayesian methods with spatial modeling techniques, focusing on Bayesian Conditional Autoregressive (CAR) models implemented using the CARBayes and INLA packages. We will demonstrate the superiority of Bayesian spatial models in analyzing spatially correlated data compared to traditional approaches, showing their ability to capture complex spatial relationships and provide more accurate predictions.

## Methodology

### Background: Areal Data and neighborhood structure

Areal data differs from point data, which consists of measurements from a known set of geo-spatial points. The boundary of areas can be considered polygons determined by a closed sequence of ordered coordinates connected by straight line segments. 

The essential idea is that the probability of values estimated at any given location are conditional on the level of neighboring values. We assume that nearby locations tend to have similar characteristics or behaviors. Therefore, we need to define what is nearby locations.

### Neighborhood Structure

The most important component of spatial models is the neighborhood structure. What does it mean for a region to be considered a "neighbor" of another region? The most important part, and the reason why we include a neighborhood structure, is that "neighbors" are correlated with each other. The value of outcome variable $y_i$ can be explained in part by the value of $y$ amongst the neighbors of region $y_i$. Most often, a Queen neighborhood structure is used. This means that if two regions touch at any point, they are considered neighbors. Without a large amount of data context, this is a fine assumption. We will utilize a Queen neighborhood structure in our analysis. Below we examine the structure of $W$, which represents the neighborhood structure of our data.

For a set of $N$ areal units, the relationship between areal units is described by an $n \times n$ adjacency matrix $W$. The entries indicate whether two regions $n_i$ and $n_j$ are neighbors, with a value of 1 signifying adjacency and 0 indicating non-adjacency. It's worth noting that in models like Conditional Autoregressive (CAR) models, the neighbor relationship is symmetric, but a region is not considered its own neighbor ($W_{ii} = 0$).
  
```{r echo=FALSE, fig.width=3, fig.height= 4}
# neighborhood structure
centroids <- st_centroid(st_geometry(simple_data_A), of_largest_polygon=TRUE)
W.nb <- poly2nb(simple_data_A, simple_data_A$grid_id) # using Queen
nb_Q_net <- nb2lines(nb = W.nb, coords = centroids, as_sf = TRUE)

simple_data_A %>%
ggplot() + 
  geom_sf(fill = 'white',color = 'darkgrey') + 
  geom_sf(data = centroids) + 
  geom_sf(data = nb_Q_net) + 
  theme_classic()+
  labs(caption = "Visualization of Neighbor Connections")+
   theme(
    plot.caption = element_text(color = "grey50")  # Adjust caption text color
  )



```
  
### Conditional Autoregressive (CAR) priors for spatial random effects


$\phi$, an n-length vector $\phi = (\phi_1, ..., \phi_n)^T$, is the spatial random variables that can represent spatial interactions between $n_i$ and $n_j$.

In the full conditional distribution, each $\phi_i$ is conditional on the sum of the weighted values of its neighbors, and has unknown variance.
$$
\begin{split}
 
 \phi_i | \phi_j ,j \neq i, 
 &\sim N (\sum\limits_{j=1}^n b_{ij} \phi_j, \tau_i^{-1})\\ 
 
\end{split}
$$
Where $\tau_i$ is a spatially varying precision parameter, and $b_{ii} = 0$.

The summation term in this expression is simply the weighted sum of the mean adjusted $\rho$ at all other locations $j$ - this may or may not be a reasonable assumption for a particular problem under consideration.

By Brook's Lemme, the joint distribution fo $\phi$ is then:
$$
\begin{split}
 \phi &\sim N(0, [D_{\tau}(I - \alpha B)]^{-1})\\
 \end{split}
$$

then the CAR prior specification simplifies to:

$$
\begin{split}
 \phi &\sim N(0, [\tau (D- \alpha W)]^{-1})\\
 \end{split}
$$


#### Assumption:

Where $W$ is the $n \times n$ adjacency matrix where entries $\{i,i\}$ are zero and the off-diagonal elements are 1 if regions $i$ and $j$ are neighbors and 0.

$D$ is the $n \times n$ diagonal matrix where entries $\{i,i\}$ are the number of neighbors of region $i$ and the off-diagonal entries are 0.

$D_{\tau} = \tau D$

$\alpha$ controls the amount of spatial correlation parameter that determines the size and nature (positive or negative) of the spatial neighborhood effect; $\alpha = 0$ implies spatial independence and $\alpha = 1$ implies complete spatial correlation. 

$B$ is the scaled adjacency matrix $D^{-1}W$

$I$ is an $n \times n$ identity matrix
 
When $\alpha$ is in the interval $(0,1)$, the covariance matrix $[D_{\tau}(I - \alpha B)]$ is positive definite, thus the joint distribution $\phi$ is proper.


###  Likelihood Function: A Gaussian Specification 

Suppose we have aggregated continuous data $y_1,y_2,...,y_n$ at $n$ locations, and we expect that neighboring locations will have similar data With a Gaussian likelihood:

$$
\begin{split}
y_i \sim \text{N}\left(X_i \beta + \phi_i \right)
\end{split}
$$

Where $X_i$ is a design vector (the $i^{th}$ row from a design matrix), $\beta$ is a vector of coefficients, $\phi_i$ is a spatial adjustment.
  
  


### Complete Bayesian Specification

Our posterior distribution is:
$$
\begin{split}
p(\phi, \beta, \alpha, \tau \mid y) \propto p(y \mid \beta, \phi) p(\phi \mid \alpha, \tau) p(\alpha) p(\tau) p(\beta)
\end{split}
$$

### MCMC

MCMC, or Markov Chain Monte Carlo, is a computational technique sampling from complex probability distributions. By constructing a Markov chain with a stationary distribution equivalent to the target posterior distribution, MCMC iteratively explores the parameter space, generating samples that approximate the desired distribution. The Metropolis-Hastings algorithm, an important MCMC method, proposes new states based on a proposal distribution and acceptance probability.


### INLA

The Integrated Nested Laplace Approximation (INLA) is another method to approximate posterior distributions. It is an alternative to using Monti Carlo Markov Chains (MCMC). The main conceptual difference between the two methodologies is that INLA attempts to approximate marginal posterior distributions for each parameter, whereas MCMC approximates the joint posterior. Because of this, INLA is more computationally efficient, but can be less accurate in some cases. INLA requires that models be expressed as Gaussian Markov Random Fields (GMRF). We approximate our posterior model using both MCMC and INLA methods later on [@INLAreference]. We use the INLA package

## Data Analysis

Our research delves into the spatial dynamics of economic activity using geographically gridded economic data from the G-Econ Project at Yale University, provided by SEDAC for the years 1990, 1995, and 2000. Focusing on modeling Gross Cell Product (GCP), akin to Gross Domestic Product (GDP), we aim to understand the clustering tendencies and geographic attributes of economic productivity. Given the concentration of economic activity in urban centers and coastal regions, studying GCP patterns is crucial for informing policies and strategies aimed at promoting equitable and sustainable economic development.

#### Research Question
How do the spatial patterns of Gross Cell Product (GCP) evolve across different regions, and what extent is this variation influenced by population density?

#### Data Overview
  
```{r echo=FALSE}

pre_log <- simple_data_A %>% filter(POPGPW_2005_40 !=0) %>% 
ggplot(aes(x=MER2005_40))+
  geom_density()+
  labs( x = 'Gross Cell Product')

post_log <-simple_data_A %>% filter(POPGPW_2005_40 !=0) %>% 
ggplot(aes(x=log(MER2005_40)))+
  geom_density()+
  labs( x = 'log(Gross Cell Product)')

ggarrange(pre_log, post_log, 
          nrow = 1, ncol = 2, labels = c("Pre log","Post log"))

pre_map <- mapview(simple_data_A, zcol = "MER2005_40", legend = TRUE,layer.name = "GCT")

real <- mapview(simple_data_A, zcol = "log(MER2005_40)", legend = TRUE,layer.name = "log(GCT)")

pre_post <- leafsync::sync(pre_map,real, ncol = 2)
pre_post
```

```{r include=FALSE}
# ggpairs(data = simple_data_A, columns = c(3,9,12,13,14))
```

### GLM result 

```{r echo=FALSE}

# form <- `log(MER2005_40)`  ~ dis_to_water + log(POPGPW_2005_40) + TEMPAV_8008 + D3
# use simpler model
form <- `log(MER2005_40)`  ~ log(POPGPW_2005_40) 
lm_mod <- lm(formula=form, data = simple_data_A)
summary(lm_mod)

BIC(lm_mod)

simple_data_A$lm_pred <- predict(lm_mod)
# 
# simple_data_A %>% ggplot() +
#   geom_sf(aes(fill = lm_pred)) +
#   scale_fill_gradient2(mid = "white", high = "red", low = "blue") + 
#   theme_classic()


# refined scale 

```

### Moran's I to Test the Spacial Autocorrelation

Moran's I is a measure of spatial autocorrelation, commonly used in spatial statistics to assess the degree of clustering or dispersion of a variable across a geographic area. It quantifies the extent to which similar values of a variable are clustered together in space.

```{r echo=FALSE}
W.list <- nb2listw(W.nb, style="B", zero.policy = TRUE) # list with spatial weights for W.nb

moran.mc(x=residuals(lm_mod), listw=W.list, nsim = 1000)
```
The Moranâ€™s I test has a p-value much less than 0.05, which suggests that the residuals contain substantial positive spatial autocorrelation.
  
## SIMULATION

### CARBayes

We used the CARBayes package, developed by [@CARBayespackage].

Check the Simulation for Car Model

```{r cache=TRUE, include= FALSE }
W <- nb2mat(W.nb, style="B")

# simulation
chain <- S.CARleroux(formula = form, data=simple_data_A, family="gaussian", W = W,
      burnin=1000, n.sample=10000, n.chains=3, n.cores=3)
```




```{r echo=FALSE}
# check simulation
plot1 <- plot(chain$samples$phi[, 2:4][, 1], main = "Trace Plot of Phi")
plot(chain$samples$beta[,2], main = "Trace Plot of beta")
plot(chain$samples$rho, main = "Trace Plot of rho")
plot(chain$samples$tau2, main = "Trace Plot of tau")


# saveRDS()

```

### CAR Result
```{r}
# coefficient summary
chain$summary.results

```

```{r include=FALSE}
y.fit <- chain$samples$fitted
y.fit <- t(t(y.fit))

simple_data_A$CAR <- apply(y.fit, 2, median)
simple_data_A$LL <- apply(y.fit, 2, quantile, 0.025)
simple_data_A$UL <- apply(y.fit, 2, quantile, 0.975)

simple_data_A$dis_to_water <- simple_data_A$dis_to_water
simple_data_A$pop <- simple_data_A$`log(POPGPW_2005_40)`
simple_data_A$temp <- simple_data_A$TEMPAV_8008
simple_data_A$GCP <- simple_data_A$`log(MER2005_40)`
simple_data_A$D3 <- simple_data_A$D3
simple_data_A$lm_pred <- simple_data_A$lm_pred

at <- seq(min(c(simple_data_A$CAR, simple_data_A$LL, simple_data_A$UL,simple_data_A$lm_pred)),
          max(c(simple_data_A$CAR, simple_data_A$LL, simple_data_A$UL,simple_data_A$lm_pred)),
          length.out = 8)


popuptable <- leafpop::popupTable(dplyr::mutate_if(simple_data_A,
                                  is.numeric, round, digits = 2),
zcol = c("pop","GCP", "temp","D3", "dis_to_water","LL", "UL","CAR"),
row.numbers = FALSE, feature.id = FALSE)


m1 <- mapview(simple_data_A, zcol = "CAR", map.types = "CartoDB.Positron",
              at = at, popup = popuptable, legend = TRUE,layer.name = "CAR")

# m2 <- mapview(simple_data_A, zcol = "LL", map.types = "CartoDB.Positron",
#               at = at, popup = popuptable)
# m3 <- mapview(simple_data_A, zcol = "UL", map.types = "CartoDB.Positron",
#               at = at, popup = popuptable)

lm_model <- mapview(simple_data_A, zcol = "lm_pred", map.types = "CartoDB.Positron", at = at, popup = popuptable, legend = TRUE,layer.name = "GLM")

real <- mapview(simple_data_A, zcol = "log(MER2005_40)", map.types = "CartoDB.Positron",
              at = at, popup = popuptable, legend = TRUE,layer.name = "log(GCP)")


```


```{r include=FALSE}
# INLA


nb2INLA("map.adj", W.nb)
g <- inla.read.graph(filename = "map.adj")

formula <- `log(MER2005_40)` ~ log(POPGPW_2005_40) 
  f(re_u, model = "besag", graph = g, scale.model = TRUE)
  
res <- inla(formula, family = "gaussian", data = simple_data_A,
control.predictor = list(compute = TRUE),
control.compute = list(return.marginals.predictor = TRUE))


# Posterior mean and 95% CI
simple_data_A$PMINLA <- res$summary.fitted.values[, "mean"]
# simple_data_A$LLINLA <- res$summary.fitted.values[, "0.025quant"]
# simple_data_A$ULINLA <- res$summary.fitted.values[, "0.975quant"]

minla <- mapview(simple_data_A, zcol = "PMINLA", map.types = "CartoDB.Positron",
              at = at, popup = popuptable, legend = TRUE,layer.name = "INLA")
```

### INLA result:(haven't done it yet)

## Final Prediction (Real data, GLM, CAR, INLA )
```{r echo=FALSE}
# all map
m <- leafsync::sync(real,lm_model, m1, minla, ncol = 2) 

m
```


## Reference


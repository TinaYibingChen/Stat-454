---
title: "454 project progress report 2"
author: "Tina Chen, Kyle Suelflow, Samina Stack"
date: "`r format(Sys.Date(),'%e %B, %Y')`"
output:
   prettydoc::html_pretty:
    theme: cayman
    toc: yes
bibliography: Library.bib
    
---

```{r setup, include=FALSE, warning=FALSE}

knitr::opts_chunk$set(
  collapse = TRUE, 
  warning = FALSE,
  message = FALSE,
  fig.height = 2.75, 
  fig.width = 4.25,
  fig.align = 'center')

source('Cleaning.R')
```

## Introduction 

In many statistical analyses, the assumption of independently and identically distributed (I.I.D) observations is a common starting point. However, this assumption may not always hold true, particularly in contexts where observations exhibit spatial correlation. For example, we might expect that production patterns look more similar for two households in Minnesota and Wisconsin than households in Minnesota and Virginia because Minnesota and Wisconsin have similar climates, and exogenous factors such as weather may impact the production process. In such scenarios, traditional statistical models may fail to capture the underlying spatial dependencies present in the data.

To address this challenge, spatial models offer a solution by explicitly incorporating spatial relationships into the modeling framework. These models acknowledge that observations in nearby locations tend to be more similar to each other than those in distant locations. By accounting for spatial correlation, spatial models provide a more nuanced understanding of the data and can yield more accurate predictions and inferences.

Bayesian statistical methods serve as a generally more intuitive and flexible alternative to the more standard frequentist methods. They account for prior knowledge, not just the available data, when generating predictions. This facilitates a refinement process as new data becomes available, allowing us to update our predictions using previous predictions and new data. This process also allows for more intuitive interpretation of probabilities as relative plausibility an event occurs rather than the frequency at which it occurs.

In our paper, we want to investigate the integration of Bayesian methods with spatial modeling techniques, focusing on Bayesian Conditional Autoregressive (CAR) models implemented using the CARBayes and INLA packages. We will demonstrate these methods using economic data from the western US, and evaluate similarlities and differences between methods.

## Methodology

### Background: Areal Data and neighborhood structure

Areal data differs from point data, which consists of measurements from a known set of geospatial points. The boundary of areas can be considered polygons determined by a closed sequence of ordered coordinates connected by straight line segments. 

The essential idea is that the probability of values estimated at any given location is conditional on the level of neighboring values. We assume that nearby locations tend to have similar characteristics or behaviors. Therefore, we need to define the nearby locations.

### Neighborhood Structure

The most important component of spatial models is the neighborhood structure. What does it mean for a region to be considered a "neighbor" of another region? The most important part, and the reason why we include a neighborhood structure, is that "neighbors" are correlated with each other. The value of outcome variable $Y_i$ can be explained in part by the value of $Y$ amongst the neighbors of region $Y_i$. Most often, a Queen neighborhood structure is used. This means that if two regions touch at any point, they are considered neighbors. Without a large amount of data context, this is a fine assumption. We will utilize a Queen neighborhood structure in our analysis. Below, we examine the structure of $W$, which represents the neighborhood structure of our data.

For a set of $N$ areal units, the relationship between areal units is described by an $n \times n$ adjacency matrix $W$. The entries indicate whether two regions $n_i$ and $n_j$ are neighbors, with a value of 1 signifying adjacency and 0 indicating non-adjacency. It's worth noting that in models like Conditional Autoregressive (CAR) models, the neighbor relationship is symmetric, but a region is not considered its own neighbor ($W_{ii} = 0$).
  
```{r echo=FALSE, fig.width=3, fig.height= 4}
# neighborhood structure
centroids <- st_centroid(st_geometry(simple_data_A), of_largest_polygon=TRUE)
W.nb <- poly2nb(simple_data_A, simple_data_A$grid_id) # using Queen
nb_Q_net <- nb2lines(nb = W.nb, coords = centroids, as_sf = TRUE)

simple_data_A %>%
ggplot() + 
  geom_sf(fill = 'white',color = 'darkgrey') + 
  geom_sf(data = centroids) + 
  geom_sf(data = nb_Q_net) + 
  theme_classic()+
  labs(caption = "Visualization of Neighbor Connections")+
   theme(
    plot.caption = element_text(color = "grey50") 
  )

```
  


###  Likelihood Function: A Gaussian Specification 

Suppose we have aggregated continuous data $Y_1, Y_2, \ldots, Y_n$ at $n$ locations, and we expect that neighboring locations will have similar data With a *Gaussian likelihood*.

The spatial variation in the response is modeled by a matrix of covariates $X$ and a spatial structure component $\phi$, the latter of which is included to model any spatial autocorrelation that remains in the data after the covariate effects have been accounted for. 

$$
\begin{split}
Y_i &\sim \mathcal{N}(\mu_i, \nu^2) \\
\text{where} \quad \mu_i &= X_i \beta + \phi_i \\
\text{for} \quad i &= 1, \ldots, n 
\end{split}
$$

Where

 * $Y_i$ represents a random variable that follows a Gaussian distribution distributed with mean $\mu_i$ and variance $\nu^2$.
 
 * $\beta$ is a $p$-length vector of coefficients
 
 * $X_i$, is a $n \times p$  matrix of predictors (covariates) associated with the each of the areal units, the first column corresponds to an intercept term. $p$ refers to the number of coefficients. 
 
 
 * $\phi$, an n-length vector $\phi = (\phi_1, ..., \phi_n)^T$, is the spatial random variables that can represent spatial interactions between $n_i$ and $n_j$.
  
  


### Instinct Conditional Autoregressive (ICAR) priors for spatial random effects

The spatial structure component $\phi_i$ can be written as $\phi \sim \text{N}(0, \tau^2 Q(W)^{-1})$. This formulation captures the spatial autocorrelation structure of the data by incorporating the spatial precision matrix $Q$ and the variance parameter $\tau^2$. It assumes that the spatial random effects follow a multivariate Gaussian distribution with mean 0 and covariance matrix $\tau^2 Q(W)^{-1}$. The precision matrix $Q$ controls the spatial autocorrelation structure of the random effects, based on the non-negative symmetric adjacency matrix $W$.


$$
\begin{split}
\phi &\sim \text{MVN}(0, \tau^2 Q(W)^{-1})
\end{split}
$$

An Intrinsic Conditional Auto-Regressive (ICAR) model assumes a complete spatial correlation between regions. In the ICAR model, the precision matrix $Q(W)$ can be defined as:

$$
\begin{split}
Q(W) &= D-\rho W\\
\text{Where} \;\; \rho &=1 \;\; \text{in ICAR}
\end{split}
$$
 
 Where
 
 * $W$ is the $n \times n$ adjacency matrix where entries $\{i,i\}$ are zero and the off-diagonal elements are 1 if regions $i$ and $j$ are neighbors and 0.

 * $D$ is the $n \times n$ diagonal matrix where entries $\{i,i\}$ are the number of neighbors of region $i$ and the off-diagonal entries are 0.
 
 * The parameter $\rho$ controls the strength of spatial autocorrelation. Based on the assumption of the complete spatial correlation between regions, it is 1 in ICAR.

In the context of ICAR, the matrix $Q$ is singular, meaning it cannot be used directly to model the data in a Frequentist approach. However, it can be used as a prior within a hierarchical Bayesian model by imposing a constraint that ensures the sum of each row equals zero.

The corresponding conditional distribution specification for $\phi$ is:

$$
\begin{split}
\phi_i|\phi_{-i}, W, \tau^2, \rho &\sim \mathcal{N}\left(\frac{\rho\sum_{j=1}^n w_{ij} \phi_j}{\rho\sum_{j=1}^n w_{ij} + 1 - \rho}, \frac{\tau^2}{\rho\sum_{j=1}^n w_{ij} + 1 - \rho}\right) \\  
\text{Where} \; \rho &= 1 \; \; \text{in ICAR}
\end{split}
$$

The conditional expectation is calculated as the average of the random effects in neighboring areas, while the conditional variance is inversely proportional to the number of neighbors. This approach is suitable because when random effects exhibit strong spatial autocorrelation, areas with more neighbors benefit from increased information about their random effect values from neighboring areas. Consequently, this increased information reduces uncertainty [@CARBayespackage]. 




### Complete Bayesian Model

$$
\begin{split}
\text{Likelihood} \;\; Y_i &\sim \text{N}(\mu_i , \nu^2) \quad  \text{where } \;\; \mu_i = X_i \beta + \phi_i \\
\text{Prior} \;\; \beta &\sim \text{N}(\mu_\beta, \Sigma_\beta) \\
\phi_i|\phi_{-i}, W, \tau^2, &\sim \text{N}\left(\frac{\sum_{j=1}^n w_{ij} \phi_j}{\sum_{j=1}^n w_{ij}}, \frac{\tau^2}{\sum_{j=1}^n w_{ij} \phi_j}\right) \\
\nu^2 &\sim \text{Inverse-Gamma}(1, 0.01) \\
\tau^2 &\sim \text{Inverse-Gamma}(1, 0.01) \\
\end{split}
$$

The posterior distribution of the parameters $\beta$, $\phi$, $\nu^2$, and $\tau^2$ given the observed data $Y_i$ can be expressed as:

$$
\begin{split}
p(\phi, \beta, \tau^2 ,\nu^2 \mid Y_i) &\propto prior \cdot likelihood \\
&= p(\beta) p(\phi \mid  \tau^2) p(\tau^2)  p(\nu^2) p(Y_i \mid \beta, \phi, \nu^2)
\end{split}
$$

## Data Analysis

To demonstrate the methodology described above, we analyze the spatial dynamics of economic activity using geographically gridded economic data from the G-Econ Project at Yale University, provided by SEDAC for the years 1990, 1995, and 2000. Specifically, we look at how the spatial patterns of Gross Cell Product (GCP), akin to Gross Domestic Product (GDP), evolve across different regions, and to what extent this variation may be influenced by population density. Focusing on modeling GCP, we aim to understand the clustering tendencies and geographic attributes of economic productivity. Given the concentration of economic activity in urban centers and coastal regions, studying GCP patterns is crucial for informing policies and strategies aimed at promoting equitable and sustainable economic development.

### Research Question
How do the spatial patterns of Gross Cell Product (GCP) evolve across different regions, and what extent is this variation influenced by population density?

### Data Overview

Our main variables of interest in this analysis are GCP (called GCT in the data) and population density. As we see from the density plots below, GCP is heavily right-skewed with a mode around zero so we log it to make it more normally distributed. We can see from the plot above that in the far Western United States, GCP tends to be highest around metropolitan areas like Seattle, San Francisco, and Los Angeles. Metropolitan areas are generally defined as having a large population nucleus of at least 50,000 people, and surrounding areas with a high degree of economic and social integration with that nucleus (CITE). This supports our interest in population density as a predictor in our model. Though population density for a given metropolitan area may be offset by the area's size, the two should be highly correlated. This implies that population density may be similarly related to GCP, especially considering our constant grid size.
  
```{r echo=FALSE, fig.width = 10, fig.height = 6}

# pre_log <- simple_data_A %>% 
#   filter(POPGPW_2005_40 !=0) %>% 
#   ggplot(aes(x=MER2005_40))+
#     geom_density()+
#     labs( x = 'Gross Cell Product')

post_log <-simple_data_A%>% 
  filter(POPGPW_2005_40 !=0) %>% 
  ggplot(aes(x=log(MER2005_40)))+
  geom_density()+
  labs(x = 'log(Gross Cell Product)', y = "Density", title = "Distribution of the logged Gross Cell Product (GCP)")+
  theme_classic()
# ggarrange(pre_log, post_log, 
#           nrow = 1, ncol = 2, labels = c("Pre log","Post log"))

# pre_map <- mapview(simple_data_A, zcol = "MER2005_40", legend = TRUE,layer.name = "GCT")

real <- mapview(simple_data_A, zcol = "log(MER2005_40)", legend = TRUE,layer.name = "log(GCT)")

# pre_post <- leafsync::sync(pre_map,real, ncol = 2)
# pre_post
post_log
real
```


### GLM result 

```{r echo=FALSE}

# form <- `log(MER2005_40)`  ~ dis_to_water + log(POPGPW_2005_40) + TEMPAV_8008 + D3
# use simpler model
form <- `log(MER2005_40)`  ~ log(POPGPW_2005_40) 
lm_mod <- lm(formula=form, data = simple_data_A)
sum <- summary(lm_mod)
coefs <- as.data.frame(sum$coefficients)



simple_data_A$lm_pred <- predict(lm_mod)
# 
# simple_data_A %>% ggplot() +
#   geom_sf(aes(fill = lm_pred)) +
#   scale_fill_gradient2(mid = "white", high = "red", low = "blue") + 
#   theme_classic()


# refined scale 

```

### Moran's I to Test the Spacial Autocorrelation

Moran's I is a measure of spatial autocorrelation, commonly used in spatial statistics to assess the degree of clustering or dispersion of a variable across a geographic area. It quantifies the extent to which similar values of a variable are clustered together in space. 

```{r echo=FALSE}
W.list <- nb2listw(W.nb, style="B", zero.policy = TRUE) # list with spatial weights for W.nb

moran <- moran.mc(x=residuals(lm_mod), listw=W.list, nsim = 1000)
moran_df <- data.frame(p_value = moran$p.value, statistic = moran$statistic)

knitr::kable(moran_df, format = "html")
```
The Moran’s I test has a p-value much less than 0.05, which suggests that the residuals contain substantial positive spatial autocorrelation.
  
## SIMULATION

In order to approximate the posterior distribution of bayesian spatial models, we need to use simulation techniques. We explore two of these techniques, MCMC and INLA, below.

### MCMC

MCMC, or Markov Chain Monte Carlo, is a computational technique sampling from complex probability distributions. By constructing a Markov chain with a stationary distribution equivalent to the target posterior distribution, MCMC iteratively explores the parameter space, generating samples that approximate the desired distribution. The Metropolis-Hastings algorithm, an important MCMC method, proposes new states based on a proposal distribution and acceptance probability.

### INLA

The Integrated Nested Laplace Approximation (INLA) is another method to approximate posterior distributions. It is an alternative to using Monti Carlo Markov Chains (MCMC). The main conceptual difference between the two methodologies is that INLA attempts to approximate marginal posterior distributions for each parameter, whereas MCMC approximates the joint posterior. Because of this, INLA is more computationally efficient, but can be less accurate in some cases. INLA requires that models be expressed as Gaussian Markov Random Fields (GMRF). We approximate our posterior model using both MCMC and INLA methods later on [@INLAreference]. 

### CARBayes

We used the CARBayes package, developed by [@CARBayespackage]. Shown below are trace and density plots for $\beta$, $\phi$, and $\tau^2$. The trace plots show that each chain is exploring similar parts of the distribution, and that the distributions are normal. This is good!

```{r include= FALSE }
W <- nb2mat(W.nb, style="B")

# simulation, set rho = 1 
chain <- S.CARleroux(formula = form, data=simple_data_A, family="gaussian", W = W,rho=1,
      burnin=1000, n.sample=10000, n.chains=3, n.cores=3)
```

```{r echo=FALSE, fig.width = 10, fig.height = 5}
#Phi Data
phi_1 <- chain$samples$phi[, 2:4][, 1][[1]]
phi_2 <- chain$samples$phi[, 2:4][, 1][[2]]
phi_3 <- chain$samples$phi[, 2:4][, 1][[3]]
iteration <- 1:9000
phi_data <- data.frame(chain_1 = phi_1, chain_2 = phi_2, chain_3 = phi_3, iteration = iteration)

#Phi TracePlot
phi_trace <- phi_data%>%
  ggplot(aes(x = iteration))+
  geom_line(aes(y = var1, color = "#9A49EB"))+
  geom_line(aes(y = var1.1, color = "#97A5EA"))+
  geom_line(aes(y = var1.2, color = "#EBD849"))+
  theme_classic()+
  labs(x = "Iteration", y = "Phi", title = "Phi Trace Plot with 3 Chains")+
  theme(legend.position = "none")

#Phi Density
phi_density <- phi_data%>%
  pivot_longer(names_to = "Chain", values_to = "Estimate", cols = !iteration)%>%
  ggplot(aes(x = Estimate))+
  geom_density()+
  theme_classic()+
  labs(x = "Phi", y = "Density", title = "Density Plot of Phi across 3 Chains")

ggarrange(phi_trace, phi_density,
          nrow = 1, ncol = 2)

#Beta Data
beta_1 <- chain$samples$beta[,2][[1]]
beta_2 <- chain$samples$beta[,2][[2]]
beta_3 <- chain$samples$beta[,2][[3]]
beta_data <- data.frame(chain_1 = beta_1, chain_2 = beta_2, chain_3 = beta_3, iteration = iteration)

#Beta traceplot
beta_trace <- beta_data%>%
  ggplot(aes(x = iteration))+
  geom_line(aes(y = var1, color = "#9A49EB"))+
  geom_line(aes(y = var1.1, color = "#97A5EA"))+
  geom_line(aes(y = var1.2, color = "#EBD849"))+
  theme_classic()+
  labs(x = "Iteration", y = "Beta", title = "Beta Trace Plot with 3 Chains")+
  theme(legend.position = "none")

#Beta Density
beta_density <- beta_data%>%
  pivot_longer(names_to = "Chain", values_to = "Estimate", cols = !iteration)%>%
  ggplot(aes(x = Estimate))+
  geom_density()+
  theme_classic()+
  labs(x = "Beta", y = "Density", title = "Density Plot of Beta across 3 Chains")

ggarrange(beta_trace, beta_density,
          nrow = 1, ncol = 2)

#Tau Data
tau_1 <- chain$samples$tau2[[1]]
tau_2 <- chain$samples$tau2[[2]]
tau_3 <- chain$samples$tau2[[3]]
tau_data <- data.frame(chain_1 = tau_1, chain_2 = tau_2, chain_3 = tau_3, iteration = iteration)

#Tau Traceplot
tau_trace <- tau_data%>%
  ggplot(aes(x = iteration))+
  geom_line(aes(y = chain_1, color = "#9A49EB"))+
  geom_line(aes(y = chain_2, color = "#97A5EA"))+
  geom_line(aes(y = chain_3, color = "#EBD849"))+
  theme_classic()+
  labs(x = "Iteration", y = "Tau", title = "Tau Trace Plot with 3 Chains")+
  theme(legend.position = "none")

#Tau Density
tau_density <- tau_data%>%
  pivot_longer(names_to = "Chain", values_to = "Estimate", cols = !iteration)%>%
  ggplot(aes(x = Estimate))+
  geom_density()+
  theme_classic()+
  labs(x = "Tau", y = "Density", title = "Density Plot of Tau across 3 Chains")

ggarrange(tau_trace, tau_density,
          nrow = 1, ncol = 2)
```

```{r echo=FALSE}
# coefficient summary
chains <- chain$summary.results
chains_df <- as.data.frame(chains)

params <- chains_df[3:5,]%>%
  mutate(sd = (Mean - `2.5%`)/2)%>%
  select(Mean, sd)

knitr::kable(params, format = "html")
```

The parameters $\phi$, $\nu^2$, and $\tau^2$ and their mean estimate are shown above.

```{r include=FALSE}
y.fit <- chain$samples$fitted
y.fit <- t(t(y.fit))

simple_data_A$CAR <- apply(y.fit, 2, median)
simple_data_A$LL <- apply(y.fit, 2, quantile, 0.025)
simple_data_A$UL <- apply(y.fit, 2, quantile, 0.975)

simple_data_A$dis_to_water <- simple_data_A$dis_to_water
simple_data_A$pop <- simple_data_A$`log(POPGPW_2005_40)`
simple_data_A$temp <- simple_data_A$TEMPAV_8008
simple_data_A$GCP <- simple_data_A$`log(MER2005_40)`
simple_data_A$D3 <- simple_data_A$D3
simple_data_A$lm_pred <- simple_data_A$lm_pred

at <- seq(min(c(simple_data_A$CAR, simple_data_A$LL, simple_data_A$UL,simple_data_A$lm_pred)),
          max(c(simple_data_A$CAR, simple_data_A$LL, simple_data_A$UL,simple_data_A$lm_pred)),
          length.out = 8)


popuptable <- leafpop::popupTable(dplyr::mutate_if(simple_data_A,
                                  is.numeric, round, digits = 2),
zcol = c("pop","GCP", "temp","D3", "dis_to_water","LL", "UL","CAR"),
row.numbers = FALSE, feature.id = FALSE)


m1 <- mapview(simple_data_A, zcol = "CAR", map.types = "CartoDB.Positron",
              at = at, popup = popuptable, legend = TRUE,layer.name = "CAR")

# m2 <- mapview(simple_data_A, zcol = "LL", map.types = "CartoDB.Positron",
#               at = at, popup = popuptable)
# m3 <- mapview(simple_data_A, zcol = "UL", map.types = "CartoDB.Positron",
#               at = at, popup = popuptable)

lm_model <- mapview(simple_data_A, zcol = "lm_pred", map.types = "CartoDB.Positron", at = at, popup = popuptable, legend = TRUE,layer.name = "GLM")

real <- mapview(simple_data_A, zcol = "log(MER2005_40)", map.types = "CartoDB.Positron",
              at = at, popup = popuptable, legend = TRUE,layer.name = "log(GCP)")


```

### Results

Shown below are the coefficient estimates for $\beta_0$ and $\beta_1$, across the 3 methods we used to estimate them: GLM, MCMC, and INLA.

```{r echo=FALSE}
# INLA
nb2INLA("map.adj", W.nb)
g <- inla.read.graph(filename = "map.adj")

simple_data_A$re_u <- 1:nrow(simple_data_A)

formula <- `log(MER2005_40)` ~ log(POPGPW_2005_40) + f(re_u, model = "besag", graph = g, scale.model = TRUE)

res <- inla(formula, family = "gaussian", data = simple_data_A,
control.predictor = list(compute = TRUE),
control.compute = list(return.marginals.predictor = TRUE))


INLAcoef <- res$summary.fixed

CAR_betas <- chains_df[1:2,]%>%
  mutate(sd = (Mean - `2.5%`)/2)%>%
  select(Mean, sd)%>%
  mutate(Method = "MCMC")
INLA_betas <- INLAcoef%>%
  select(mean, sd)%>%
  mutate(Method = "INLA")%>%
  mutate(Mean = mean)%>%
  select(Mean, sd, Method)
glm_betas <- coefs%>%
  mutate(Mean = Estimate,
         sd = `Std. Error`, 
         Method = "GLM")%>%
  select(Mean, sd, Method)
  
betas <- rbind(glm_betas, CAR_betas, INLA_betas)

knitr::kable(betas, format = "html")

# Posterior mean and 95% CI
simple_data_A$PMINLA <- res$summary.fitted.values[, "mean"]
# simple_data_A$LLINLA <- res$summary.fitted.values[, "0.025quant"]
# simple_data_A$ULINLA <- res$summary.fitted.values[, "0.975quant"]

minla <- mapview(simple_data_A, zcol = "PMINLA", map.types = "CartoDB.Positron",
              at = at, popup = popuptable, legend = TRUE,layer.name = "INLA")
```

Interestingly, GLM and INLA are virtually identical with the estimates they give, while MCMC is slightly different. 

```{r echo=FALSE}
MAE_MCMC_calculation <- 1/length(simple_data_A$CAR)*(sum(simple_data_A$`log(MER2005_40)`- simple_data_A$CAR))
MAE_GLM_calculation <- 1/length(simple_data_A$lm_pred)*(sum(simple_data_A$`log(MER2005_40)`- simple_data_A$lm_pred))
MAE_INLA_calculation <- 1/length(simple_data_A$PMINLA)*(sum(simple_data_A$`log(MER2005_40)`- simple_data_A$PMINLA))


MAEdf <- data.frame(MAE = c(MAE_INLA_calculation, MAE_GLM_calculation, MAE_MCMC_calculation), Method = c("INLA", "GLM", "MCMC"))
```

## Final Prediction (Real data, GLM, CAR, INLA )
```{r echo=FALSE}
# all map
m <- leafsync::sync(real,lm_model, m1, minla, ncol = 2) 
m
```


### Limitation

Spatial modeling with the ICAR model presents several challenges that can impact its effectiveness. The ICAR model assumes spatial stationarity, which implies that the strength of spatial dependence remains constant across the entire study area. This may not hold true in regions with diverse spatial patterns, leading to inaccurate estimations of spatial relationships. To enhance spatial modeling, the BYM (Besag, York, and Mollié) model offers a solution by integrating structured (ICAR component) and unstructured random effects, enabling more accurate predictions in complex spatial settings. Additionally, the ICAR model struggles with sparse data or irregular spatial patterns, resulting in unstable estimates and unreliable predictions. Implementing the ICAR model can be computationally intensive, especially for large datasets or complex spatial structures. The need to compute and manipulate spatial adjacency matrices for potentially numerous spatial units adds to the model's complexity. Alternative Bayesian inference methods like INLA (Integrated Nested Laplace Approximation) provide efficient approximations of posterior distributions suitable for large datasets, which makes it suitable for large datasets and complex models where MCMC might be computationally prohibitive. However,, its accuracy can depend on the quality of the Laplace approximation and the complexity of the model, while MCMC directly samples from the posterior distribution of model parameters.

## Conclusion
  Bayesian spatial modeling combines the interpretability and adaptiblity of Bayesian models with the ability to capture spatial relationships. The provides a more flexible framework for analyzing geographical data than traditional spatial modelling. Though our data analysis failed to demonstrate meaningful differences in results between our different models, our theoretical construction of the ICAR model indicates that it should produce more accurate and reliable results than a GLM model due to the regard for correlation between grids. Our models used only one predictor variable, due to the strength of the predictive ability of population density and because of the limited variables in our data. Population density was the only predictor for which our MCMC simulations were stable. It should be noted that, no matter how flexible a model is, whether a given model is most appropriate depends largely on the context of a given analysis. For example, if we have a large amount of data and do not expect to need to incorporate any in the future, a frequentist model may be sufficient. However, understanding the potential tradeoffs of these models informs more precise analysis.


## Reference


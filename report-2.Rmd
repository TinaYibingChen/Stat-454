---
title: "454 project progress report 2"
author: "Tina Chen, Kyle Suelflow, Samina Stack"
date: "`r format(Sys.Date(),'%e %B, %Y')`"
output:
   prettydoc::html_pretty:
    theme: cayman
    toc: yes
bibliography: Library.bib
    
---

```{r setup, include=FALSE, warning=FALSE}

knitr::opts_chunk$set(
  collapse = TRUE, 
  warning = FALSE,
  message = FALSE,
  fig.height = 2.75, 
  fig.width = 4.25,
  fig.align = 'center')

source('Cleaning.R')
```

## Introduction 

In many statistical analyses, the assumption of independently and identically distributed (IID) observations is a common starting point. However, this assumption may not always hold true, particularly in contexts where observations exhibit spatial correlation. For example, we might expect that production patterns look more similar for two households in Minnesota and Wisconsin than households in Minnesota and Virginia because Minnesota and Wisconsin have similar climates, and exogenous factors such as weather may impact the production process. In such scenarios, traditional statistical models may fail to capture the underlying spatial dependencies present in the data.

To address this challenge, spatial models offer a solution by explicitly incorporating spatial relationships into the modeling framework. These models acknowledge that observations in nearby locations tend to be more similar to each other than those in distant locations. By accounting for spatial correlation, spatial models provide a more nuanced understanding of the data and can yield more accurate predictions and inferences.

Bayesian statistical methods serve as a generally more intuitive and flexible alternative to the more standard frequentist methods. They account for prior knowledge, not just the available data, when generating predictions. This facilitates a refinement process as new data becomes available, allowing us to update our predictions using previous predictions and new data. This process also allows for more intuitive interpretation of probabilities as relative plausibility an event occurs rather than the frequency at which it occurs.

In our paper, we want to investigate the integration of Bayesian methods with spatial modeling techniques, focusing on Bayesian Conditional Autoregressive (CAR) models implemented using the CARBayes and INLA packages. We will demonstrate the superiority of Bayesian spatial models in analyzing spatially correlated data compared to traditional approaches, showing their ability to capture complex spatial relationships and provide more accurate predictions.

## Methodology

### Background: Areal Data and neighborhood structure

Areal data differs from point data, which consists of measurements from a known set of geospatial points. The boundary of areas can be considered polygons determined by a closed sequence of ordered coordinates connected by straight line segments. 

The essential idea is that the probability of values estimated at any given location is conditional on the level of neighboring values. We assume that nearby locations tend to have similar characteristics or behaviors. Therefore, we need to define the nearby locations.

### Neighborhood Structure

The most important component of spatial models is the neighborhood structure. What does it mean for a region to be considered a "neighbor" of another region? The most important part, and the reason why we include a neighborhood structure, is that "neighbors" are correlated with each other. The value of outcome variable $Y_i$ can be explained in part by the value of $Y$ amongst the neighbors of region $Y_i$. Most often, a Queen neighborhood structure is used. This means that if two regions touch at any point, they are considered neighbors. Without a large amount of data context, this is a fine assumption. We will utilize a Queen neighborhood structure in our analysis. Below, we examine the structure of $W$, which represents the neighborhood structure of our data.

For a set of $N$ areal units, the relationship between areal units is described by an $n \times n$ adjacency matrix $W$. The entries indicate whether two regions $n_i$ and $n_j$ are neighbors, with a value of 1 signifying adjacency and 0 indicating non-adjacency. It's worth noting that in models like Conditional Autoregressive (CAR) models, the neighbor relationship is symmetric, but a region is not considered its own neighbor ($W_{ii} = 0$).
  
```{r echo=FALSE, fig.width=3, fig.height= 4}
# neighborhood structure
centroids <- st_centroid(st_geometry(simple_data_A), of_largest_polygon=TRUE)
W.nb <- poly2nb(simple_data_A, simple_data_A$grid_id) # using Queen
nb_Q_net <- nb2lines(nb = W.nb, coords = centroids, as_sf = TRUE)

simple_data_A %>%
ggplot() + 
  geom_sf(fill = 'white',color = 'darkgrey') + 
  geom_sf(data = centroids) + 
  geom_sf(data = nb_Q_net) + 
  theme_classic()+
  labs(caption = "Visualization of Neighbor Connections")+
   theme(
    plot.caption = element_text(color = "grey50") 
  )

```
  


###  Likelihood Function: A Gaussian Specification 

Suppose we have aggregated continuous data $Y_1, Y_2, \ldots, Y_n$ at $n$ locations, and we expect that neighboring locations will have similar data With a *Gaussian likelihood*.

The spatial variation in the response is modeled by a matrix of covariates $X$ and a spatial structure component $\phi$, the latter of which is included to model any spatial autocorrelation that remains in the data after the covariate effects have been accounted for. 

$$
\begin{split}
Y_i &\sim \mathcal{N}(\mu_i, \nu^2) \\
\text{where} \quad \mu_i &= X_i \beta + \phi_i \\
\text{for} \quad i &= 1, \ldots, n 
\end{split}

$$

Where

 * $Y_i$ represents a random variable that follows a Gaussian distribution distributed with mean $\mu_i$ and variance $\nu^2$.
 
 * $\beta$ is a $p$-length vector of coefficients
 
 * $X_i$, is a $n \times p$  matrix of predictors (covariates) associated with the each of the areal units, the first column corresponds to an intercept term. $p$ refers to the number of coefficients. 
 
 
 * $\phi$, an n-length vector $\phi = (\phi_1, ..., \phi_n)^T$, is the spatial random variables that can represent spatial interactions between $n_i$ and $n_j$.
  
  


### Instinct Conditional Autoregressive (ICAR) priors for spatial random effects

The spatial structure component $\phi_i$ can be written as $\phi \sim \text{N}(0, \tau^2 Q(W)^{-1})$. This formulation captures the spatial autocorrelation structure of the data by incorporating the spatial precision matrix $Q$ and the variance parameter $\tau^2$. It assumes that the spatial random effects follow a multivariate Gaussian distribution with mean 0 and covariance matrix $\tau^2 Q(W)^{-1}$. The precision matrix $Q$ controls the spatial autocorrelation structure of the random effects, based on the non-negative symmetric adjacency matrix $W$.


$$
\begin{split}
\phi &\sim \text{MVN}(0, \tau^2 Q(W)^{-1})
\end{split}
$$

An Intrinsic Conditional Auto-Regressive (ICAR) model assumes a complete spatial correlation between regions. In the ICAR model, the precision matrix $Q(W)$ can be defined as:

$$
\begin{split}
Q(W) &= D-\rho W\\
\text{Where} \;\; \rho &=1 \;\; \text{in ICAR}
\end{split}
$$
 
 Where
 
 * $W$ is the $n \times n$ adjacency matrix where entries $\{i,i\}$ are zero and the off-diagonal elements are 1 if regions $i$ and $j$ are neighbors and 0.

 * $D$ is the $n \times n$ diagonal matrix where entries $\{i,i\}$ are the number of neighbors of region $i$ and the off-diagonal entries are 0.
 
 * The parameter $\rho$ controls the strength of spatial autocorrelation. Based on the assumption of the complete spatial correlation between regions, it is 1 in ICAR.

In the context of ICAR, the matrix $Q$ is singular, meaning it cannot be used directly to model the data in a Frequentist approach. However, it can be used as a prior within a hierarchical Bayesian model by imposing a constraint that ensures the sum of each row equals zero.

The corresponding conditional distribution specification for $\phi$ is:

$$
\begin{split}
\phi_i|\phi_{-i}, W, \tau^2, \rho &\sim \mathcal{N}\left(\frac{\rho\sum_{j=1}^n w_{ij} \phi_j}{\rho\sum_{j=1}^n w_{ij} + 1 - \rho}, \frac{\tau^2}{\rho\sum_{j=1}^n w_{ij} + 1 - \rho}\right) \\  
\text{Where} \; \rho &= 1 \; \; \text{in ICAR}
\end{split}

$$

The conditional expectation is calculated as the average of the random effects in neighboring areas, while the conditional variance is inversely proportional to the number of neighbors. This approach is suitable because when random effects exhibit strong spatial autocorrelation, areas with more neighbors benefit from increased information about their random effect values from neighboring areas. Consequently, this increased information reduces uncertainty [@CARBayespackage]. 




### Complete Bayesian Model

$$
\begin{split}
\text{Likelihood} \;\; Y_i &\sim \text{N}(\mu_i , \nu^2) \quad  \text{where } \;\; \mu_i = X_i \beta + \phi_i \\

\text{Prior} \;\; \beta &\sim \text{N}(\mu_\beta, \Sigma_\beta) \\

\phi_i|\phi_{-i}, W, \tau^2, &\sim \text{N}\left(\frac{\sum_{j=1}^n w_{ij} \phi_j}{\sum_{j=1}^n w_{ij}}, \frac{\tau^2}{\sum_{j=1}^n w_{ij} \phi_j}\right) \\

\nu^2 &\sim \text{Inverse-Gamma}(1, 0.01) \\
\tau^2 &\sim \text{Inverse-Gamma}(1, 0.01) \\
\end{split}
$$

The posterior distribution of the parameters $\beta$, $\phi$, $\nu^2$, and $\tau^2$ given the observed data $Y_i$ can be expressed as:

$$
\begin{split}
p(\phi, \beta, \tau^2 ,\nu^2 \mid Y_i) &\propto prior \cdot likelihood \\
&= p(\beta) p(\phi \mid  \tau^2) p(\tau^2)  p(\nu^2) p(Y_i \mid \beta, \phi, \nu^2)
\end{split}
$$





### MCMC

MCMC, or Markov Chain Monte Carlo, is a computational technique sampling from complex probability distributions. By constructing a Markov chain with a stationary distribution equivalent to the target posterior distribution, MCMC iteratively explores the parameter space, generating samples that approximate the desired distribution. The Metropolis-Hastings algorithm, an important MCMC method, proposes new states based on a proposal distribution and acceptance probability.


### INLA

The Integrated Nested Laplace Approximation (INLA) is another method to approximate posterior distributions. It is an alternative to using Monti Carlo Markov Chains (MCMC). The main conceptual difference between the two methodologies is that INLA attempts to approximate marginal posterior distributions for each parameter, whereas MCMC approximates the joint posterior. Because of this, INLA is more computationally efficient, but can be less accurate in some cases. INLA requires that models be expressed as Gaussian Markov Random Fields (GMRF). We approximate our posterior model using both MCMC and INLA methods later on [@INLAreference]. 

## Data Analysis

Our research delves into the spatial dynamics of economic activity using geographically gridded economic data from the G-Econ Project at Yale University, provided by SEDAC for the years 1990, 1995, and 2000. Focusing on modeling Gross Cell Product (GCP), akin to Gross Domestic Product (GDP), we aim to understand the clustering tendencies and geographic attributes of economic productivity. Given the concentration of economic activity in urban centers and coastal regions, studying GCP patterns is crucial for informing policies and strategies aimed at promoting equitable and sustainable economic development.

### Research Question
How do the spatial patterns of Gross Cell Product (GCP) evolve across different regions, and what extent is this variation influenced by population density?

### Data Overview
  
```{r echo=FALSE}

pre_log <- simple_data_A %>% filter(POPGPW_2005_40 !=0) %>% 
ggplot(aes(x=MER2005_40))+
  geom_density()+
  labs( x = 'Gross Cell Product')

post_log <-simple_data_A %>% filter(POPGPW_2005_40 !=0) %>% 
ggplot(aes(x=log(MER2005_40)))+
  geom_density()+
  labs( x = 'log(Gross Cell Product)')

ggarrange(pre_log, post_log, 
          nrow = 1, ncol = 2, labels = c("Pre log","Post log"))

pre_map <- mapview(simple_data_A, zcol = "MER2005_40", legend = TRUE,layer.name = "GCT")

real <- mapview(simple_data_A, zcol = "log(MER2005_40)", legend = TRUE,layer.name = "log(GCT)")

pre_post <- leafsync::sync(pre_map,real, ncol = 2)
pre_post
```

```{r include=FALSE}
# ggpairs(data = simple_data_A, columns = c(3,9,12,13,14))
```

### GLM result 

```{r echo=FALSE}

# form <- `log(MER2005_40)`  ~ dis_to_water + log(POPGPW_2005_40) + TEMPAV_8008 + D3
# use simpler model
form <- `log(MER2005_40)`  ~ log(POPGPW_2005_40) 
lm_mod <- lm(formula=form, data = simple_data_A)
summary(lm_mod)

BIC(lm_mod)


simple_data_A$lm_pred <- predict(lm_mod)
# 
# simple_data_A %>% ggplot() +
#   geom_sf(aes(fill = lm_pred)) +
#   scale_fill_gradient2(mid = "white", high = "red", low = "blue") + 
#   theme_classic()


# refined scale 

```

### Moran's I to Test the Spacial Autocorrelation

Moran's I is a measure of spatial autocorrelation, commonly used in spatial statistics to assess the degree of clustering or dispersion of a variable across a geographic area. It quantifies the extent to which similar values of a variable are clustered together in space.

```{r echo=FALSE}
W.list <- nb2listw(W.nb, style="B", zero.policy = TRUE) # list with spatial weights for W.nb

moran.mc(x=residuals(lm_mod), listw=W.list, nsim = 1000)
```
The Moran’s I test has a p-value much less than 0.05, which suggests that the residuals contain substantial positive spatial autocorrelation.
  
## SIMULATION

### CARBayes

We used the CARBayes package, developed by [@CARBayespackage].

Check the Simulation for Car Model

```{r include= FALSE }
W <- nb2mat(W.nb, style="B")

# simulation, set rho = 1 
chain <- S.CARleroux(formula = form, data=simple_data_A, family="gaussian", W = W,rho=1,
      burnin=1000, n.sample=10000, n.chains=3, n.cores=3)



```




```{r echo=FALSE}
# check simulation
plot1 <- plot(chain$samples$phi[, 2:4][, 1], main = "Trace Plot of Phi")
plot(chain$samples$beta[,2], main = "Trace Plot of beta")
plot(chain$samples$tau2, main = "Trace Plot of tau")


# saveRDS()

```

### CAR Result
```{r}
# coefficient summary
chain$summary.results

```


```{r include=FALSE}
y.fit <- chain$samples$fitted
y.fit <- t(t(y.fit))

simple_data_A$CAR <- apply(y.fit, 2, median)
simple_data_A$LL <- apply(y.fit, 2, quantile, 0.025)
simple_data_A$UL <- apply(y.fit, 2, quantile, 0.975)

simple_data_A$dis_to_water <- simple_data_A$dis_to_water
simple_data_A$pop <- simple_data_A$`log(POPGPW_2005_40)`
simple_data_A$temp <- simple_data_A$TEMPAV_8008
simple_data_A$GCP <- simple_data_A$`log(MER2005_40)`
simple_data_A$D3 <- simple_data_A$D3
simple_data_A$lm_pred <- simple_data_A$lm_pred

at <- seq(min(c(simple_data_A$CAR, simple_data_A$LL, simple_data_A$UL,simple_data_A$lm_pred)),
          max(c(simple_data_A$CAR, simple_data_A$LL, simple_data_A$UL,simple_data_A$lm_pred)),
          length.out = 8)


popuptable <- leafpop::popupTable(dplyr::mutate_if(simple_data_A,
                                  is.numeric, round, digits = 2),
zcol = c("pop","GCP", "temp","D3", "dis_to_water","LL", "UL","CAR"),
row.numbers = FALSE, feature.id = FALSE)


m1 <- mapview(simple_data_A, zcol = "CAR", map.types = "CartoDB.Positron",
              at = at, popup = popuptable, legend = TRUE,layer.name = "CAR")

# m2 <- mapview(simple_data_A, zcol = "LL", map.types = "CartoDB.Positron",
#               at = at, popup = popuptable)
# m3 <- mapview(simple_data_A, zcol = "UL", map.types = "CartoDB.Positron",
#               at = at, popup = popuptable)

lm_model <- mapview(simple_data_A, zcol = "lm_pred", map.types = "CartoDB.Positron", at = at, popup = popuptable, legend = TRUE,layer.name = "GLM")

real <- mapview(simple_data_A, zcol = "log(MER2005_40)", map.types = "CartoDB.Positron",
              at = at, popup = popuptable, legend = TRUE,layer.name = "log(GCP)")


```



```{r include=FALSE}
# INLA
nb2INLA("map.adj", W.nb)
g <- inla.read.graph(filename = "map.adj")

formula <- `log(MER2005_40)` ~ log(POPGPW_2005_40) 
  f(re_u, model = "besag", graph = g, scale.model = TRUE)
  
res <- inla(formula, family = "gaussian", data = simple_data_A,
control.predictor = list(compute = TRUE),
control.compute = list(return.marginals.predictor = TRUE))


# Posterior mean and 95% CI
simple_data_A$PMINLA <- res$summary.fitted.values[, "mean"]
# simple_data_A$LLINLA <- res$summary.fitted.values[, "0.025quant"]
# simple_data_A$ULINLA <- res$summary.fitted.values[, "0.975quant"]

minla <- mapview(simple_data_A, zcol = "PMINLA", map.types = "CartoDB.Positron",
              at = at, popup = popuptable, legend = TRUE,layer.name = "INLA")
```

```{r}
MAE_MCMC_calculation <- 1/length(simple_data_A$CAR)*(sum(simple_data_A$`log(MER2005_40)`- simple_data_A$CAR))
MAE_GLM_calculation <- 1/length(simple_data_A$lm_pred)*(sum(simple_data_A$`log(MER2005_40)`- simple_data_A$lm_pred))
MAE_INLA_calculation <- 1/length(simple_data_A$PMINLA)*(sum(simple_data_A$`log(MER2005_40)`- simple_data_A$PMINLA))
MAE_MCMC_calculation
MAE_GLM_calculation
MAE_INLA_calculation

```


### INLA result:(haven't done it yet)


## Final Prediction (Real data, GLM, CAR, INLA )
```{r echo=FALSE}
# all map
m <- leafsync::sync(real,lm_model, m1, minla, ncol = 2) 
m
```


### Limitation


## Conclusion
  we got similar results...




## Appendenx

Here is a simple demonstration for $Y = X \beta + \phi$：

$$
\begin{split}
\begin{bmatrix}
\mu_{1} \\
\mu_{2}
\end{bmatrix}
=
\begin{bmatrix}
1 & X_{1,1} & X_{1,2} \\
1 & X_{2,1} & X_{2,2}
\end{bmatrix}
\begin{bmatrix}
\beta_{0} \\
\beta_{1} \\
\beta_{2}
\end{bmatrix}
+
\begin{bmatrix}
\phi_{1} \\
\phi_{2}
\end{bmatrix}
=
\begin{bmatrix}
\beta_{0} + \beta_{1}X_{1,1} + \beta_{2}X_{1,2} + \phi_1 \\
\beta_{0} + \beta_{1}X_{2,1} + \beta_{2}X_{2,2} + \phi_2
\end{bmatrix}
\end{split}
$$

a small demo for  Q



## Reference

